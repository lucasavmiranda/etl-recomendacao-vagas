## ğŸ“ README: Meu Primeiro Pipeline ETL (RecomendaÃ§Ã£o de Vagas)

---

## ğŸš€ O que Ã© este Projeto?

Este projeto Ã© um **Pipeline ETL** (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carregamento) que criei para praticar Engenharia de Dados com Python e a biblioteca Pandas.

O objetivo Ã© simular um sistema que **recomenda vagas de emprego** para usuÃ¡rios com base nas habilidades que eles possuem.

### ğŸŒŸ Destaques do Projeto

* **ExtraÃ§Ã£o Simples:** LÃª dados de usuÃ¡rios e vagas em formato CSV.
* **InteligÃªncia:** O sistema calcula a compatibilidade entre as *skills* do usuÃ¡rio e os requisitos da vaga.
* **OrganizaÃ§Ã£o:** O cÃ³digo Ã© dividido em mÃ³dulos (`extract`, `transform`, `load`) para ser fÃ¡cil de entender e manter.
* **Reaproveitamento:** Garante que a mesma recomendaÃ§Ã£o nÃ£o seja enviada duas vezes ao mesmo usuÃ¡rio (IdempotÃªncia).

---

## âš™ï¸ Estrutura do Projeto

A estrutura foi organizada da seguinte forma:

ETL-RECOMENDACAO-VAGAS/ â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ jobs.csv # Vagas disponÃ­veis (Entrada) â”‚ â”œâ”€â”€ user_news.csv # HistÃ³rico de RecomendaÃ§Ãµes (SaÃ­da) â”‚ â””â”€â”€ users.csv # Perfil dos UsuÃ¡rios (Entrada) â””â”€â”€ src/ â”œâ”€â”€ config.py # Guarda configuraÃ§Ãµes (Ex: nota mÃ­nima para recomendar) â”œâ”€â”€ extract.py # Lida com a leitura dos CSVs â”œâ”€â”€ load.py # Salva o resultado final no user_news.csv â”œâ”€â”€ transform.py # ContÃ©m a lÃ³gica de cÃ¡lculo do score â””â”€â”€ utils.py # FunÃ§Ãµes auxiliares (como normalizar as skills)


### ğŸ”„ Como o Pipeline Funciona?

1.  **ExtraÃ§Ã£o:** O sistema lÃª as habilidades dos usuÃ¡rios e as habilidades requeridas pelas vagas.
2.  **TransformaÃ§Ã£o:** Ele limpa as habilidades (ex: transforma "SQL" e "sql" em "sql") e usa a lÃ³gica de *scoring* para calcular quantos pontos cada usuÃ¡rio tem para cada vaga. 

[Image of a data pipeline diagram showing Extract, Transform, and Load steps, with the Transform step highlighting AI and GPT-4 integration]

3.  **Carregamento:** As recomendaÃ§Ãµes mais relevantes sÃ£o salvas no arquivo `user_news.csv` para uso futuro.

---

## ğŸ’» Guia de ExecuÃ§Ã£o (Google Colab)

Para rodar este projeto, vocÃª precisa montar a mesma estrutura de pastas no ambiente do Colab e fazer o upload dos arquivos.

### 1. PreparaÃ§Ã£o do Ambiente

Crie um novo Notebook e execute esta cÃ©lula primeiro:

```python
# CÃ©lula 1: ConfiguraÃ§Ã£o do Ambiente
import sys
import pandas as pd

# CriaÃ§Ã£o das pastas 'data' e 'src'
!mkdir -p data
!mkdir -p src

# Adiciona a pasta 'src' ao caminho do Python para que as importaÃ§Ãµes funcionem
if 'src' not in sys.path:
    sys.path.append('src')

print("Ambiente configurado. Agora, faÃ§a o upload dos seus arquivos.")
2. Upload
Use o painel de Arquivos (na lateral esquerda) para:

Subir seus arquivos da pasta src/ para a pasta src no Colab.

Subir seus arquivos users.csv e jobs.csv para a pasta data no Colab.

3. ExecuÃ§Ã£o
Crie uma nova cÃ©lula no Colab e execute a funÃ§Ã£o principal:

Python

# CÃ©lula 2: ExecuÃ§Ã£o do Pipeline
from src.config import Config
from src.pipeline import run_pipeline # Se sua funÃ§Ã£o principal estiver em pipeline.py

# ... (Seu cÃ³digo completo deve estar no Notebook antes desta cÃ©lula) ...

# Roda o pipeline e exibe as Ãºltimas recomendaÃ§Ãµes
output_df = run_pipeline() 
print("\n--- Resultado do Carregamento (Ãšltimas Mensagens) ---")
print(output_df.tail(10))
ğŸ§‘â€ğŸ“ ConclusÃ£o e PrÃ³ximos Passos
Ao desenvolver este projeto, eu me concentrei em construir um pipeline que fosse claro, funcional e que usasse boas prÃ¡ticas, mesmo sendo iniciante. Estruturei o cÃ³digo em mÃ³dulos e garanto que o sistema Ã© robusto o suficiente para nÃ£o duplicar dados e lidar com a entrada de forma limpa.

Eu transformei um desafio de cÃ³digo em um projeto prÃ¡tico de portfÃ³lio que demonstra meu entendimento do ciclo ETL e minha habilidade de criar soluÃ§Ãµes modulares.

ğŸ”— Links
Notebook Colab: https://colab.research.google.com/drive/16i64d_kGY7PkzT8uwhz4y_nA67cVg_BU

RepositÃ³rio GitHub: https://github.com/lucasavmiranda/etl-recomendacao-vagas